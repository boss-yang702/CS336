{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60637c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'basics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 从 basics 导入必要的组件\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset  \u001b[38;5;66;03m# 确保 data.py 文件在您的项目中\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_entropy, clip_gradient\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbasics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasicTransformerLM  \u001b[38;5;66;03m# 确保 model.py 文件在您的项目中\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'basics'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from transformers.hf_argparser import HfArgumentParser\n",
    "import logging\n",
    "\n",
    "# 设置日志记录\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 从 basics 导入必要的组件\n",
    "from basics.data import Dataset  # 确保 data.py 文件在您的项目中\n",
    "from basics.nn_utils import cross_entropy, clip_gradient\n",
    "from basics.model import BasicTransformerLM  # 确保 model.py 文件在您的项目中\n",
    "from basics.optimizer import AdamW, get_cosine_lr\n",
    "\n",
    "# 训练配置 (与 run_train.sh 中的参数类似)\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    dataset_name: str = field(default=\"tinystory\")\n",
    "    vocab_size: int = field(default=10000)\n",
    "    context_length: int = field(default=256)\n",
    "    batch_size: int = field(default=16)  # 每个 GPU 的 batch size\n",
    "    d_model: int = field(default=512)\n",
    "    num_layers: int = field(default=4)\n",
    "    num_heads: int = field(default=16)\n",
    "    d_ff: int = field(default=1344)\n",
    "    total_iters: int = field(default=20000)\n",
    "    max_learning_rate: float = field(default=5e-4)\n",
    "    cosine_cycle_iters: int = field(default=20000)\n",
    "    weight_decay: float = field(default=0.001)\n",
    "    device: str = field(default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    wandb_logging: bool = field(default=False)\n",
    "    wandb_project: Optional[str] = field(default=\"cs336-assignment1\")\n",
    "    wandb_run_name: Optional[str] = field(default=\"tinystories-ddp\")\n",
    "    eval_interval: int = field(default=200)\n",
    "    log_interval: int = field(default=20)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.warmup_iters = int(self.total_iters * 0.01)\n",
    "        if self.wandb_logging:\n",
    "            assert self.wandb_project is not None, 'wandb_project must be provided if wandb_logging is True'\n",
    "            assert self.wandb_run_name is not None, 'wandb_run_name must be provided if wandb_logging is True'\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'  # 选择一个未使用的端口\n",
    "\n",
    "    # 初始化进程组\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)  # 使用 \"nccl\" 作为后端\n",
    "    torch.cuda.set_device(rank)  # 将当前进程绑定到相应的 GPU\n",
    "    logging.info(f\"Rank {rank}: 已初始化进程组，使用设备: cuda:{rank}\")\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    logging.info(f\"Rank {dist.get_rank()}: 已销毁进程组\")\n",
    "\n",
    "def main(rank, world_size, config):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # 每个进程使用其 rank 确定的随机种子\n",
    "    torch.manual_seed(42 + rank)\n",
    "    \n",
    "    # 在当前进程使用的 GPU 上创建模型\n",
    "    model = BasicTransformerLM(**asdict(config)).to(config.device)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # 加载数据集，每个进程加载全部数据，但使用 DistributedSampler 进行划分\n",
    "    dataset = Dataset(**asdict(config))\n",
    "    train_sampler = DistributedSampler(dataset.train_data, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    val_sampler = DistributedSampler(dataset.val_data, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "    \n",
    "    train_loader = DataLoader(dataset.train_data, batch_size=config.batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset.val_data, batch_size=config.batch_size, sampler=val_sampler)\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = AdamW(ddp_model.parameters(), **asdict(config))\n",
    "\n",
    "    # 训练循环\n",
    "    iter_num = 0\n",
    "    while iter_num < config.total_iters:\n",
    "        train_sampler.set_epoch(iter_num)  # 设置 sampler 的 epoch, 以确保 shuffle\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = dataset.get_batch_from_data(batch, config.context_length)  # 使用新的 get_batch_from_data\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "            logits = ddp_model(x)\n",
    "            loss = cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "            clip_gradient(ddp_model.parameters(), 1.0)\n",
    "            lr = get_cosine_lr(iter_num, **asdict(config))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            optimizer.step()\n",
    "\n",
    "            # 日志记录 (仅在 rank 0 上记录)\n",
    "            if iter_num % config.log_interval == 0 and rank == 0:\n",
    "                logging.info(f'Iter: {iter_num}, Train loss: {loss.item():.4f}, LR: {lr:.6f}')\n",
    "\n",
    "            # 评估 (仅在 rank 0 上评估)\n",
    "            if iter_num % config.eval_interval == 0 and rank == 0:\n",
    "                eval(ddp_model, val_loader, dataset, config, iter_num, lr)  # 修改为接受 val_loader\n",
    "\n",
    "            iter_num += 1\n",
    "            if iter_num >= config.total_iters:\n",
    "                break  # 达到总迭代次数后退出\n",
    "    cleanup()  # 完成后清理\n",
    "\n",
    "# 添加一个从原始数据获取 batch 的函数\n",
    "def get_batch_from_data(data: torch.Tensor, context_length: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # 随机选择起始索引，确保有足够的长度\n",
    "    start_idx = torch.randint(0, len(data) - context_length, (1,)).item()\n",
    "    x = data[start_idx:start_idx + context_length].long()  # 转换为 Long 类型\n",
    "    y = data[start_idx + 1:start_idx + 1 + context_length].long()\n",
    "    return x, y\n",
    "\n",
    "def eval(model, val_loader, dataset, config, iter_num, lr):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  # 遍历 val_loader\n",
    "            x, y = dataset.get_batch_from_data(batch, config.context_length)  # 使用 get_batch_from_data\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "            logits = model(x)\n",
    "            loss = cross_entropy(logits, y)\n",
    "            total_loss += loss.item()\n",
    "    total_loss /= len(val_loader)\n",
    "    logging.info(f'Iter: {iter_num}, Val loss: {total_loss:.4f}, LR: {lr:.6f}')\n",
    "    model.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 解析配置\n",
    "    parser = HfArgumentParser(TrainingConfig)\n",
    "    config = parser.parse_args_into_dataclasses()[0]\n",
    "\n",
    "    # 设置 world_size (使用所有可用的 GPU)\n",
    "    world_size = torch.cuda.device_count()\n",
    "    logging.info(f\"使用 {world_size} 个 GPU 进行训练\")\n",
    "    \n",
    "    # 使用 torch.multiprocessing.spawn 启动多个进程\n",
    "    torch.multiprocessing.spawn(main, args=(world_size, config), nprocs=world_size, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    # 初始化进程组，backend可改为'nccl'如果用GPU\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def distributed_demo(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 这里如果用GPU，建议设置设备：torch.cuda.set_device(rank)\n",
    "    \n",
    "    data = torch.randint(0, 10, (3,))\n",
    "    print(f\"Rank {rank} data (before all-reduce): {data}\")\n",
    "    # all_reduce累加所有进程数据，结果同步到每个进程\n",
    "    dist.all_reduce(data, async_op=False)\n",
    "    print(f\"Rank {rank} data (after all-reduce): {data}\")\n",
    "    \n",
    "    dist.destroy_process_group()  # 结束进程组，防止资源泄漏\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "    mp.spawn(fn=distributed_demo, args=(world_size,), nprocs=world_size, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0bce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7041, 0.2253, 0.4310, 0.8126, 0.5045])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
